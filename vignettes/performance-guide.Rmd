---
title: "HDPS Performance Guide"
author: "Min Fan"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{HDPS Performance Guide}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8,
  fig.height = 6
)
```

# HDPS Performance Guide

This vignette provides comprehensive guidance on optimizing HDPS performance for different dataset sizes and computing environments.

## Performance Characteristics

The HDPS package is optimized for high-performance processing with the following characteristics:

- **Processing Speed**: 100+ covariates per second
- **Memory Efficiency**: Minimal memory overhead with smart copying
- **Scalability**: Linear scaling with dataset size
- **Parallel Processing**: Multi-core support for large datasets

## Dataset Size Recommendations

### Small Datasets (< 1,000 patients)
```{r small-datasets}
library(hdps)
library(data.table)

# For small datasets, use single-core processing
result <- hdps(
  data = your_data,
  id_col = "patient_id",
  code_col = "diagnosis_code",
  exposure_col = "treatment",
  outcome_col = "outcome",
  n_cores = 1,  # Single core is sufficient
  batch_size = 25
)
```

### Medium Datasets (1,000 - 10,000 patients)
```{r medium-datasets}
# For medium datasets, use moderate parallel processing
result <- hdps(
  data = your_data,
  id_col = "patient_id", 
  code_col = "diagnosis_code",
  exposure_col = "treatment",
  outcome_col = "outcome",
  n_cores = 2,  # 2-4 cores recommended
  batch_size = 50
)
```

### Large Datasets (> 10,000 patients)
```{r large-datasets}
# For large datasets, use full parallel processing
result <- hdps(
  data = your_data,
  id_col = "patient_id",
  code_col = "diagnosis_code", 
  exposure_col = "treatment",
  outcome_col = "outcome",
  n_cores = 4,  # 4+ cores recommended
  batch_size = 100,
  progress = TRUE  # Enable progress tracking
)
```

## Memory Optimization

### Preprocessing Large Datasets
```{r memory-optimization}
# Convert to data.table for better performance
if (!is.data.table(your_data)) {
  your_data <- as.data.table(your_data)
}

# Remove unnecessary columns before processing
essential_cols <- c("patient_id", "diagnosis_code")
your_data <- your_data[, essential_cols, with = FALSE]

# Process in chunks for very large datasets
chunk_size <- 50000
if (nrow(your_data) > chunk_size) {
  # Process in chunks
  chunks <- split(1:nrow(your_data), 
                  ceiling(seq_along(1:nrow(your_data)) / chunk_size))
  
  results <- lapply(chunks, function(chunk_idx) {
    chunk_data <- your_data[chunk_idx]
    hdps(chunk_data, "patient_id", "diagnosis_code", 
         "treatment", "outcome", n_cores = 2)
  })
}
```

## Parallel Processing Tuning

### Optimal Core Count
```{r core-tuning}
# Determine optimal core count
available_cores <- parallel::detectCores()
optimal_cores <- min(available_cores - 1, 8)  # Leave 1 core free, max 8

cat("Available cores:", available_cores, "\n")
cat("Recommended cores:", optimal_cores, "\n")
```

### Batch Size Optimization
```{r batch-tuning}
# Batch size depends on dataset size and available memory
n_covariates <- 1000
n_patients <- 10000

# Small batches for memory-constrained environments
batch_size_small <- min(25, n_covariates / 4)

# Large batches for memory-rich environments  
batch_size_large <- min(100, n_covariates / 2)

cat("Small batch size:", batch_size_small, "\n")
cat("Large batch size:", batch_size_large, "\n")
```

## Performance Monitoring

### Timing Your Analysis
```{r performance-monitoring}
# Monitor performance
start_time <- Sys.time()
start_memory <- gc()[, 2]

result <- hdps(
  data = your_data,
  id_col = "patient_id",
  code_col = "diagnosis_code",
  exposure_col = "treatment", 
  outcome_col = "outcome",
  n_cores = 4,
  progress = TRUE
)

end_time <- Sys.time()
end_memory <- gc()[, 2]

# Performance metrics
duration <- as.numeric(difftime(end_time, start_time, units = "secs"))
memory_used <- end_memory - start_memory
processing_rate <- nrow(result$prioritization) / duration

cat("Duration:", round(duration, 2), "seconds\n")
cat("Memory used:", round(memory_used, 1), "MB\n") 
cat("Processing rate:", round(processing_rate, 0), "covariates/second\n")
```

## Troubleshooting Performance Issues

### Common Issues and Solutions

1. **Out of Memory Errors**
   - Reduce `batch_size`
   - Use fewer `n_cores`
   - Process data in chunks

2. **Slow Processing**
   - Increase `n_cores` (up to available cores)
   - Increase `batch_size` (if memory allows)
   - Ensure data is `data.table` format

3. **Progress Bar Not Showing**
   - Set `progress = TRUE`
   - Ensure sufficient data for progress tracking

### Performance Profiling
```{r profiling}
# Profile your analysis
library(profvis)

profvis({
  result <- hdps(
    data = your_data,
    id_col = "patient_id",
    code_col = "diagnosis_code", 
    exposure_col = "treatment",
    outcome_col = "outcome",
    n_cores = 2
  )
})
```

## Best Practices

1. **Data Preparation**
   - Convert to `data.table` format
   - Remove unnecessary columns
   - Ensure consistent data types

2. **Parameter Tuning**
   - Start with conservative settings
   - Gradually increase parallelization
   - Monitor memory usage

3. **Error Handling**
   - Use try-catch for large analyses
   - Save intermediate results
   - Implement checkpointing for very large datasets

## Example: Complete Performance-Optimized Workflow

```{r complete-example}
# Complete performance-optimized workflow
library(hdps)
library(data.table)

# 1. Data preparation
data <- as.data.table(your_data)
data <- data[!is.na(patient_id) & !is.na(diagnosis_code)]

# 2. Determine optimal parameters
n_patients <- length(unique(data$patient_id))
n_cores <- min(parallel::detectCores() - 1, 4)
batch_size <- ifelse(n_patients > 5000, 100, 50)

# 3. Run analysis with monitoring
start_time <- Sys.time()
cat("Starting HDPS analysis...\n")
cat("Patients:", n_patients, "\n")
cat("Cores:", n_cores, "\n")
cat("Batch size:", batch_size, "\n")

result <- hdps(
  data = data,
  id_col = "patient_id",
  code_col = "diagnosis_code",
  exposure_col = "treatment",
  outcome_col = "outcome", 
  n_candidates = 200,
  min_patients = 10,
  n_cores = n_cores,
  batch_size = batch_size,
  progress = TRUE
)

end_time <- Sys.time()
duration <- as.numeric(difftime(end_time, start_time, units = "secs"))

cat("Analysis completed in", round(duration, 2), "seconds\n")
cat("Candidates found:", nrow(result$candidates), "\n")
cat("Covariates prioritized:", nrow(result$prioritization), "\n")
```

This performance guide should help you optimize HDPS for your specific use case and computing environment.
